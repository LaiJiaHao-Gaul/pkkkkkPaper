{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a19374bf-e1d0-4671-a775-c5bd08ff5857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "52154c2e-d6cf-4d55-b0e1-fbe36bdff7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id                                           Question  \\\n",
      "0   1                              What is data science?   \n",
      "1   2  What are the key steps in the data science pro...   \n",
      "2   3  What is the difference between supervised and ...   \n",
      "3   4                Explain the bias-variance tradeoff.   \n",
      "4   5                       What is feature engineering?   \n",
      "\n",
      "                                              Answer  \n",
      "0  Data science is an interdisciplinary field tha...  \n",
      "1  The key steps typically include problem defini...  \n",
      "2  Supervised learning involves training a model ...  \n",
      "3  The bias-variance tradeoff is the balance betw...  \n",
      "4  Feature engineering is the process of selectin...  \n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "file_path = '/Users/coco/Downloads/learning_chatbot/data/dataset_qa.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check the dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "034f8e90-20ce-4340-b8b3-c216ae48ef5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id                                           Question  \\\n",
      "0   1                              What is data science?   \n",
      "1   2  What are the key steps in the data science pro...   \n",
      "2   3  What is the difference between supervised and ...   \n",
      "3   4                Explain the bias-variance tradeoff.   \n",
      "4   5                       What is feature engineering?   \n",
      "\n",
      "                                              Answer  \n",
      "0  Data science is an interdisciplinary field tha...  \n",
      "1  The key steps typically include problem defini...  \n",
      "2  Supervised learning involves training a model ...  \n",
      "3  The bias-variance tradeoff is the balance betw...  \n",
      "4  Feature engineering is the process of selectin...  \n"
     ]
    }
   ],
   "source": [
    "# Basic preprocessing: removing any NaNs and ensuring text is clean\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# If needed, you can clean the text data\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = text.strip()  # Remove leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "df['Question'] = df['Question'].apply(clean_text)\n",
    "df['Answer'] = df['Answer'].apply(clean_text)\n",
    "\n",
    "# Verify the cleaned data\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d2323a09-f997-43c7-b530-35bedd77a6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'prompt': 'Q: What is data science?\\nA:', 'completion': ' Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data.'}, {'prompt': 'Q: What are the key steps in the data science process?\\nA:', 'completion': ' The key steps typically include problem definition, data collection, data preparation, exploratory data analysis, modeling, evaluation, and deployment.'}, {'prompt': 'Q: What is the difference between supervised and unsupervised learning?\\nA:', 'completion': ' Supervised learning involves training a model on labeled data, where the algorithm learns the relationship between input features and target labels. Unsupervised learning deals with unlabeled data and aims to find hidden patterns or structures within the data.'}]\n"
     ]
    }
   ],
   "source": [
    "# 将问题和答案组合成适合微调的格式\n",
    "# 这将取决于具体的模型要求。这里，我们将创建一个提示和完成的列表。\n",
    "data = []\n",
    "for index, row in df.iterrows():\n",
    "    prompt = f\"Q: {row['Question']}\\nA:\"\n",
    "    completion = f\" {row['Answer']}\"\n",
    "    data.append({'prompt': prompt, 'completion': completion})\n",
    "\n",
    "# 将格式化的数据保存为JSON文件以供训练\n",
    "import json\n",
    "\n",
    "formatted_file_path = '/Users/coco/Downloads/learning_chatbot/data/formatted_qadataset.jsonl'\n",
    "with open(formatted_file_path, 'w') as outfile:\n",
    "    for entry in data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "# 确认数据已正确格式化\n",
    "print(data[:3])  # 显示前3个条目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e2b534ec-234f-4ad4-a6f2-125170c420b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "class CustomGPT2Model(GPT2LMHeadModel):\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # 使用 GPT-2 模型的父类方法进行前向传播\n",
    "        outputs = super().forward(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        print(f\"Logits shape: {logits.shape}\")\n",
    "        print(f\"Logits sample: {logits[0, -1, :5].detach().cpu().numpy()}\")\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            print(f\"Labels shape: {labels.shape}\")\n",
    "            print(f\"Labels sample: {labels[0, :5].detach().cpu().numpy()}\")\n",
    "\n",
    "            # Shift the logits and labels for calculating the loss\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "            print(f\"Shifted Logits shape: {shift_logits.shape}\")\n",
    "            print(f\"Shifted Labels shape: {shift_labels.shape}\")\n",
    "\n",
    "            try:\n",
    "                # 计算损失\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                print(f\"Calculated loss: {loss.item()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating loss: {e}\")\n",
    "\n",
    "        return {'loss': loss, 'logits': logits, 'past_key_values': outputs.past_key_values}\n",
    "# 加载预训练模型和分词器\n",
    "model_name = 'gpt2'  # 你也可以使用其他模型\n",
    "model = CustomGPT2Model.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# 添加一个新的填充标记\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 加载你格式化的数据集\n",
    "dataset = load_dataset('json', data_files=formatted_file_path, split='train')\n",
    "\n",
    "# 对数据集进行分词和填充\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['prompt'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "        pad_to_multiple_of=8,\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 微调参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    logging_dir='./logs',            # 日志保存路径\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# 初始化Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9a5a0de2-8c6d-410d-abcf-5d217906aff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([4, 512, 50258])\n",
      "Logits sample: [-105.8998  -107.28401 -108.54804 -111.86737 -105.87482]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[148], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 微调模型\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/accelerate/accelerator.py:2149\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2145\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   2148\u001b[0m     \u001b[38;5;66;03m# deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\u001b[39;00m\n\u001b[0;32m-> 2149\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\n\u001b[1;32m   2150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   2151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed_engine_wrapped\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "# 微调模型\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "da66f7c9-7af5-4717-95b5-e25b3074fc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/coco/Downloads/learning_chatbot/models/fine_tuned_model/tokenizer_config.json',\n",
       " '/Users/coco/Downloads/learning_chatbot/models/fine_tuned_model/special_tokens_map.json',\n",
       " '/Users/coco/Downloads/learning_chatbot/models/fine_tuned_model/vocab.json',\n",
       " '/Users/coco/Downloads/learning_chatbot/models/fine_tuned_model/merges.txt',\n",
       " '/Users/coco/Downloads/learning_chatbot/models/fine_tuned_model/added_tokens.json')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存微调后的模型\n",
    "model.save_pretrained('/Users/coco/Downloads/learning_chatbot/models/fine_tuned_model')\n",
    "tokenizer.save_pretrained('/Users/coco/Downloads/learning_chatbot/models/fine_tuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ac906ba9-acc5-4917-8e19-a079a344b6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 加载微调后的模型\n",
    "qa_pipeline = pipeline('text-generation', model='/Users/coco/Downloads/learning_chatbot/models/fine_tuned_model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cd0aed8d-7091-4258-b7c8-1434be14b9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the difference between classification and clustering?\n",
      "A: Q: What is the difference between classification and clustering?\n",
      "A: A clustering algorithm is a series of algorithms that perform a single task. For example, a classification algorithm may perform multiple tasks at a time, for example, sorting through an array. The classification algorithm uses a set of weights to determine which tasks are correct. For example, the algorithm can determine which images are good and which are bad. The classification algorithm can also choose which tasks are correct, for example, grouping a list of images together.\n",
      "A clustering algorithm is a set of algorithms that performs a single task. For example, a classification algorithm may perform multiple tasks at a time, for example, sorting through an array. The classification algorithm uses a set of weights\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How do you handle missing data in a dataset?\n",
      "A: Q: How do you handle missing data in a dataset?\n",
      "A: I'm not really a big fan of missing data. It's an issue of looking at what's really important. I'm not saying I don't care about that. I'm not really a huge fan of missing data. I'm just not sure how to handle it.\n",
      "But I do like to think that I can handle missing data. I don't like the idea of looking at everything and then trying to figure out how to deal with it. And I think that's a big reason I'm doing this.\n",
      "I think that the next big question is how do we solve this problem?\n",
      "Q: How do we get people to look at their data in a\n",
      "\n",
      "Q: Explain the concept of overfitting.\n",
      "A: Q: Explain the concept of overfitting.\n",
      "A: Overfitting is a term that has been around for a while. It is defined as a way of making a device that is more comfortable to use.\n",
      "A: The more comfortable you are with your device, the less comfortable it will be to use. If you have an over-fitting device, the more you will be able to use it in a way that you do not like.\n",
      "B: The more comfortable you are with your device, the less likely it will be to become uncomfortable.\n",
      "I am not saying that overfitting is the only way to improve your mobility. I am saying that it is a way that you can get more comfortable with your device.\n",
      "A:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is the difference between classification and clustering?\",\n",
    "    \"How do you handle missing data in a dataset?\",\n",
    "    \"Explain the concept of overfitting.\"\n",
    "]\n",
    "\n",
    "# 使用模型生成答案\n",
    "for question in questions:\n",
    "    prompt = f\"Q: {question}\\nA:\"\n",
    "    answers = qa_pipeline(\n",
    "    prompt, \n",
    "    max_length=150, \n",
    "    temperature=0.7, \n",
    "    top_k=50, \n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "    generated_text = answers[0]['generated_text']\n",
    "    print(f\"Q: {question}\\nA: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cee3dc-c8d0-46a9-ba01-cafb4a139d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
